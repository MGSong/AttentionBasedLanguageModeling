{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import edf\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from model2 import RNNModel\n",
    "import math\n",
    "\n",
    "train_data, trcnt = utils.load_data_onechar('data/ptb.train.txt', False)\n",
    "valid_data, vacnt = utils.load_data_onechar('data/ptb.valid.txt', False)\n",
    "test_data, tecnt = utils.load_data_onechar('data/ptb.test.txt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CalPerp(output, target, masks):\n",
    "    prob = output.gather(1, target).data#.numpy()\n",
    "    masks = masks.data#.numpy()\n",
    "    #code.interact(local=locals())\n",
    "\n",
    "    return -torch.sum(prob[(prob*masks) < 0])\n",
    "\n",
    "def Eval(data, cnt, model):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data)- batch, batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        \n",
    "        x_padded = repackage_variable(x_padded)\n",
    "        x_padded = torch.cat(x_padded, 1)\n",
    "        T = x_padded.size(0)\n",
    "        B = x_padded.size(1)\n",
    "        inp = x_padded[:T-1, :].long()\n",
    "        target = x_padded[1:, :].long().view(-1, 1)\n",
    "        \n",
    "        mask = (inp != 0).float().view(-1, 1)\n",
    "        \n",
    "        hidden = model.init_hidden(batch)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(inp, hidden)\n",
    "        output = output.view(-1, n_vocab)\n",
    "        \n",
    "        loss = output.gather(1, target) * mask\n",
    "        loss = -torch.sum(loss) / torch.sum(mask)\n",
    "        \n",
    "        avg_loss += loss\n",
    "        perp += CalPerp(output, target, mask)\n",
    "        #print(\"finish iteration\")\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "def Predict(max_step, prefix, model):\n",
    "    T = max_step       \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = prefix[t]\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "        \n",
    "        \n",
    "        hidden = model.init_hidden(1)\n",
    "        inp = Variable(torch.LongTensor([[pred]]))\n",
    "        #print(inp)\n",
    "        output, hidden = model(inp, hidden)\n",
    "        output = output.view(n_vocab)\n",
    "        #print(output)\n",
    "        pred = output.data.max(0)[1][0]\n",
    "        #print(pred)\n",
    "        \n",
    "    \n",
    "    idx = [pred for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated sentence \n",
      "the agreements bring.#{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7{\\7\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "\n",
    "def clip_gradient(model, clip):\n",
    "    \"\"\"Computes a gradient clipping coefficient based on gradient norm.\"\"\"\n",
    "    totalnorm = 0\n",
    "    for p in model.parameters():\n",
    "        modulenorm = p.grad.data.norm()\n",
    "        totalnorm += modulenorm ** 2\n",
    "    totalnorm = math.sqrt(totalnorm)\n",
    "    return min(1, clip / (totalnorm + 1e-6))\n",
    "\n",
    "def repackage_variable(v):\n",
    "    return [Variable(torch.from_numpy(h)).unsqueeze(1) for h in v]\n",
    "\n",
    "batches = range(0, len(train_data) - batch, batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "model = RNNModel(\"LSTMCell\", n_vocab, hidden_dim, hidden_dim, 1)\n",
    "#model = torch.load(\"model_att_torch.pkl\")\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix), model)\n",
    "print(\"generated sentence \")\n",
    "print(utils.to_string(generation))\n",
    "\n",
    "perp, loss = Eval(valid_data, vacnt, model)\n",
    "#print(type(perp))\n",
    "#print(type(loss))\n",
    "print(\"Initial: Perplexity: \"+str(perp) + \"Avg loss = \" + str(loss) )    \n",
    "best_loss = loss\n",
    "\n",
    "\n",
    "\n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        x_padded = repackage_variable(x_padded)\n",
    "        x_padded = torch.cat(x_padded, 1)\n",
    "        T = x_padded.size(0)\n",
    "        B = x_padded.size(1)\n",
    "        inp = x_padded[:T-1, :].long()\n",
    "        target = x_padded[1:, :].long().view(-1, 1)\n",
    "        \n",
    "        mask = (inp != 0).float().view(-1, 1)\n",
    "        \n",
    "        hidden = model.init_hidden(batch)\n",
    "        model.zero_grad()\n",
    "        #print(inp.size())\n",
    "        output, hidden = model(inp, hidden)\n",
    "        output = output.view(-1, n_vocab)\n",
    "        \n",
    "        loss = output.gather(1, target) * mask\n",
    "        loss = -torch.sum(loss) / torch.sum(mask)\n",
    "        loss.backward()\n",
    "        \n",
    "        #clipped_lr = lr * clip_gradient(model, args.clip)\n",
    "        clipped_lr = eta * clip_gradient(model, 0.5)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-clipped_lr, p.grad.data)\n",
    "        \n",
    "        \n",
    "        if k % 200 == 0:\n",
    "            #print(loss.data)\n",
    "            #output = output.max(1)[1]\n",
    "            acc = torch.sum((output.max(1)[1] == target).float() * mask) / torch.sum(mask)\n",
    "            print(\"Batch accuracy: \" + str(acc.data))\n",
    "      \n",
    "    \n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix), model)\n",
    "    print(\"generated sentence \")\n",
    "    print(utils.to_string(generation))\n",
    "\n",
    "    perp, loss = Eval(valid_data, vacnt, model)\n",
    "    print(\"Perplexity: \"+str(perp) + \"Avg loss = \" + str(loss) )\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        torch.save(model, \"model_att_torch.pkl\")\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        eta *= decay\n",
    "        model = torch.load(\"model_att_torch.pkl\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_GRU.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "# inp = edf.Value()\n",
    "\n",
    "# np.random.seed(0)\n",
    "# edf.params = []\n",
    "# C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "# Wz = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "# Wr = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "# W = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "# V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "# parameters.extend([C2V, Wz, Wr, W, V])\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "\n",
    "def GRUCell(xt, h):\n",
    "    \n",
    "    z = edf.Sigmoid(edf.VDot(edf.ConCat(xt, h), Wz))\n",
    "    r = edf.Sigmoid(edf.VDot(edf.ConCat(xt, h), Wr))\n",
    "    h_hat = edf.Tanh(edf.VDot(edf.ConCat(xt, edf.Mul(r, h)), W))\n",
    "    h_next = edf.Add(edf.Mul(z, h_hat), edf.Mul(h, edf.Add(edf.Value(1), edf.Mul(z, edf.Value(-1)))))\n",
    "    \n",
    "    return h_next\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next = GRUCell(xt, h)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next \n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next = GRUCell(xt, h)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "           \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx  \n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "#         inp.set(x_padded)\n",
    "#         loss, score = BuildModel()\n",
    "#         edf.Forward()\n",
    "#         avg_loss += loss.value\n",
    "#         perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "\n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 49.13856 Avg loss = 3.92808\n",
      "Initial generated sentence \n",
      "the agreements bringwx*//659e&@$$vl777#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u7dm77dmx37dmal77#*b/655.7qqqqqqqqyyy.wxp3u\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-598d6f86f3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zeweichu/deep-learning/pytorch/AttentionBasedLanguageModeling/edf/edf.py\u001b[0m in \u001b[0;36mForward\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Global forward/backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zeweichu/deep-learning/pytorch/AttentionBasedLanguageModeling/edf/edf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "\n",
    "def LSTMCell(xt, h, c):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wf), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wi), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wo), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.VDot(edf.ConCat(xt, h), Wc), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0]\n",
    "    T = inp.value.shape[1]\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next, c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
